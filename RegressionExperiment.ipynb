{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(dataset):\n",
    "    data = load_svmlight_file(\"./{}\".format(dataset))\n",
    "    # data = load_svmlight_file(\"../dataset/australian_scale\")\n",
    "    input_data = data[0].toarray()\n",
    "    return input_data, data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_w(dimension):\n",
    "    w = np.zeros((dimension, 1))\n",
    "    # w = np.random.random((dimension, 1))\n",
    "    # w = np.random.normal(size=(dimension, 1))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_test_loss(testing_loss, algorithm, show=False):\n",
    "    x = np.array(range(1, len(testing_loss)+1))\n",
    "    plt.figure(0)\n",
    "    # plt.plot(x, np.array(training_loss), label=\"train\")\n",
    "    plt.plot(x, np.array(testing_loss), label=algorithm)\n",
    "    if show==True:\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Experiment\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(w, X):\n",
    "    return 1.0/(1.0+np.e**(-np.dot(X, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=10000\n",
    "nEpochs=100\n",
    "lr=0.01\n",
    "# get dataset\n",
    "# x_train, x_validation, y_train, y_validation = get_dataset(input_data, label)\n",
    "x_train, y_train = get_data('a9a')\n",
    "x_validation, y_validation = get_data('a9a.t')\n",
    "\n",
    "# handle data\n",
    "temp = np.zeros((len(x_validation), 1))\n",
    "x_validation = np.column_stack((x_validation, temp))\n",
    "y_train[y_train==-1] = 0\n",
    "y_validation[y_validation==-1] = 0\n",
    "\n",
    "# handle b\n",
    "train_column = np.ones((len(x_train), 1))\n",
    "x_train = np.column_stack((x_train, train_column))\n",
    "validation_column = np.ones((len(x_validation), 1))\n",
    "x_validation = np.column_stack((x_validation, validation_column))\n",
    "\n",
    "algorithm = ['SGD', 'NAG', 'RMSProp', 'Adadelta', 'Adam']\n",
    "for a in range(len(algorithm)):\n",
    "\n",
    "    # initialize the w\n",
    "    w = initial_w(dimension=(123+1))\n",
    "\n",
    "    # plot\n",
    "    training_loss_list = []\n",
    "    validationing_loss_list = []\n",
    "\n",
    "    # number of batch\n",
    "    num_batch = int(len(x_train)/batch_size + 1)\n",
    "\n",
    "    # initialize the v(t-1) NAG\n",
    "    v_t_1 = np.zeros((123+1, 1))\n",
    "    # initialize the g(t-1) RMSProp\n",
    "    g_t_1 = np.zeros((123+1, 1))\n",
    "    # initialize the delta(t-1) AdaDelta\n",
    "    delta_t_1 = np.zeros((123+1, 1))\n",
    "    # initialize the v(t-1) and m(t-1) Adam\n",
    "    v_t_1_a = np.zeros((123+1, 1))\n",
    "    m_t_1 = np.zeros((123+1, 1))\n",
    "\n",
    "    for i in range(nEpochs):\n",
    "        training_loss = 0.0\n",
    "        validationing_loss = 0.0\n",
    "        # shuffle each batch in different epochs\n",
    "        shuffle = random.sample(range(len(x_train)), len(x_train))\n",
    "        for j in range(num_batch):\n",
    "            # batch\n",
    "            if j!=num_batch-1:\n",
    "                # the data in each batch\n",
    "                x = x_train[shuffle[j*batch_size:(j+1)*batch_size]]\n",
    "                y = y_train[shuffle[j*batch_size:(j+1)*batch_size]]\n",
    "            elif j==num_batch-1:\n",
    "                # the data in each batch\n",
    "                x = x_train[shuffle[j*batch_size:]]\n",
    "                y = y_train[shuffle[j*batch_size:]]\n",
    "\n",
    "            # algorithm\n",
    "            if algorithm[a]==\"SGD\":\n",
    "                # calculate the gradient\n",
    "                grad = (np.dot(np.transpose(x), (sigmoid(w, x) - y.reshape(-1, 1))))/len(x)\n",
    "                # update\n",
    "                w = w - lr * grad\n",
    "\n",
    "            elif algorithm[a]==\"NAG\":\n",
    "                # calculate the gradient\n",
    "                grad = (np.dot(np.transpose(x), (sigmoid((w-0.9*v_t_1), x) - y.reshape(-1, 1))))/len(x)\n",
    "                v_t = 0.9*v_t_1 + lr * grad\n",
    "                # update\n",
    "                w = w - v_t\n",
    "                v_t_1 = v_t.copy()\n",
    "\n",
    "            elif algorithm[a]==\"Adadelta\":\n",
    "                # calculate the gradient\n",
    "                grad = (np.dot(np.transpose(x), (sigmoid(w, x) - y.reshape(-1, 1))))/len(x)\n",
    "                g_t = grad.copy()\n",
    "                g_t = 0.1 * np.square(g_t_1) + 0.9 * np.square(g_t)\n",
    "\n",
    "                # update\n",
    "                delta_w = (np.sqrt(np.mean(delta_t_1)+0.001)/np.sqrt(np.mean(g_t)+0.001))*grad\n",
    "                w = w - delta_w\n",
    "                delta_t = 0.9 * delta_t_1 + 0.1 * np.square(delta_w)\n",
    "                delta_t_1 = delta_t.copy()\n",
    "\n",
    "            elif algorithm[a]==\"RMSProp\":\n",
    "                # calculate the gradient\n",
    "                grad = (np.dot(np.transpose(x), (sigmoid(w, x) - y.reshape(-1, 1))))/len(x)\n",
    "                g_t = grad.copy()\n",
    "                g_t = 0.1 * np.square(g_t_1) + 0.9 * np.square(g_t)\n",
    "\n",
    "                # update\n",
    "                w = w - (lr*grad)/np.sqrt(np.mean(g_t)+0.001)\n",
    "                g_t_1 = g_t.copy()\n",
    "\n",
    "            elif algorithm[a]==\"Adam\":\n",
    "                # calculate the gradient\n",
    "                grad = (np.dot(np.transpose(x), (sigmoid(w, x) - y.reshape(-1, 1))))/len(x)\n",
    "                m_t = 0.9 * m_t_1 + 0.1 * grad\n",
    "                v_t_a = 0.999 * v_t_1_a + 0.001 * np.square(grad)\n",
    "                m_t_hat = m_t / ( 1 - np.power(0.9, i*num_batch+j+1))\n",
    "                v_t_a_hat = v_t_a / ( 1 - np.power(0.999, i*num_batch+j+1))\n",
    "                # update\n",
    "                w = w - lr * m_t_hat / (np.sqrt(v_t_a_hat) + 1e-8)\n",
    "                m_t = m_t_hat\n",
    "                v_t_a = v_t_a_hat\n",
    "\n",
    "            # training error and validationing error\n",
    "            training_loss += -np.mean(y_train.reshape(-1, 1) * np.log(sigmoid(w, x_train)) + (1 - y_train.reshape(-1, 1)) * (np.log(1 - sigmoid(w, x_train))))\n",
    "            validationing_loss += -np.mean(y_validation.reshape(-1, 1) * np.log(sigmoid(w, x_validation)) + (1 - y_validation.reshape(-1, 1)) * (np.log(1 - sigmoid(w, x_validation))))\n",
    "\n",
    "        training_loss = training_loss/num_batch\n",
    "        validationing_loss = validationing_loss/num_batch\n",
    "        training_loss_list.append(training_loss)\n",
    "        validationing_loss_list.append(validationing_loss)\n",
    "        print(\"training error:[{}] validationing error:[{}]\".format(training_loss, validationing_loss))\n",
    "\n",
    "    # plot\n",
    "    # plot_loss(training_loss_list, validationing_loss_list)\n",
    "    if a==len(algorithm)-1:\n",
    "        plot_validation_loss(validationing_loss=validationing_loss_list, algorithm=algorithm[a], show=True)\n",
    "    else:\n",
    "        plot_validation_loss(validationing_loss=validationing_loss_list, algorithm=algorithm[a], show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
